## 文件描述符

​	文件描述符（File descriptor）是计算机科学中的一个术语，是一个用于表述指向文件的引用的抽象化概念。 文件描述符在形式上是一个非负整数。实际上，它是一个索引值，指向内核为每一个进程所维护的该进程打开文件的记录表。当程序打开一个现有文件或者创建一个新文件时，内核向进程返回一个文件描述符。在程序设计中，一些涉及底层的程序编写往往会围绕着文件描述符展开。

## BI/O描述

​	单核cpu情况下，kernel会与client端建立多个连接，每个连接其实都是一个fd（文件描述符），当有大量连接同时存在时，系统内部会启动多个线程去读取不同的fd。由于这个时期内核中的socket是阻塞的，在某个时间片上，只有某个线程在执行read，只有等read命令返回结果，另外的线程才能read，因此这个时期是同步阻塞（BI/O）的时期

![1627439886567](C:/Users/zxw/AppData/Roaming/Typora/typora-user-images/1627439886567.png)

## NI/O描述

​	上面的BI/O时期，内核中socket只能是阻塞的，因此需要启用多个线程或进程去读取不同的fd，但当内核中socket出现nonblock后，单个线程通过read读取的不会阻塞，因此可以只使用单个线程，轮询读取所有的fd，当fd没有数据时返回错误，这个时期就是同步非阻塞（NI/O）

![1627441577339](C:/Users/zxw/AppData/Roaming/Typora/typora-user-images/1627441577339.png)

## 多路复用

​	服务器端采用单线程通过 `select/poll/epoll` 等系统调用获取 fd 列表，遍历有事件的 fd 进行 `accept/recv/send` ，使其能支持更多的并发连接请求。  

### Select实现多路复用

​	NI/O的问题所在：当出现的连接越来越多时，用户进程轮询调用内核的read时成本很高，每次轮询时获取有效的fd可能比较少。为了减少用户态和内核态的切换，linux新增了一个select()函数，这个函数允许用户进程告诉内核所有的fd，然后内核返回一个有效的fd，这样用户进程无需轮询找出有效的fd，全部交给内核。

![1627453272134](C:/Users/zxw/AppData/Roaming/Typora/typora-user-images/1627453272134.png)

上面使用select()实现多路复用的问题：

- 单个进程所打开的FD是有限制的，通过 `FD_SETSIZE` 设置，默认1024 ;
- 每次调用 select，都需要把 fd 集合从用户态拷贝到内核态，这个开销在 fd 很多时会很大；
   需要维护一个用来存放大量fd的数据结构，这样会使得用户空间和内核空间在传递该结构时复制开销大 
- 对 socket 扫描时是线性扫描，采用轮询的方法，效率较低（高并发）当套接字比较多的时候，每次select()都要通过遍历FD_SETSIZE个Socket来完成调度,不管哪个Socket是活跃的,都遍历一遍。这会浪费很多CPU时间。如果能给套接字注册某个回调函数，当他们活跃时，自动完成相关操作，那就避免了轮询，这正是epoll与kqueue做的。

### epoll实现多路复用

​	为了解决上面的问题，linux内核通过mmap映射出一个共享空间，用户进程和内核都可以访问，这个共享空间中通过红黑树和链表维护了文件描述符。
​	当某一进程调用epoll_create方法时，Linux内核会创建一个eventpoll结构体，这个结构体中有两个成员与epoll的使用方式密切相关。  eventpoll结构体如下所示： 

```c
#include <sys/epoll.h>

// 数据结构
// 每一个epoll对象都有一个独立的eventpoll结构体
// 用于存放通过epoll_ctl方法向epoll对象中添加进来的事件
// epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可
struct eventpoll {
    /*红黑树的根节点，这颗树中存储着所有添加到epoll中的需要监控的事件*/
    struct rb_root  rbr;
    /*双链表中则存放着将要通过epoll_wait返回给用户的满足条件的事件*/
    struct list_head rdlist;
};

// API
int epoll_create(int size); // 内核中间加一个 ep 对象，把所有需要监听的 socket 都放到 ep 对象中
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); // epoll_ctl 负责把 socket 增加、删除到内核红黑树
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);// epoll_wait 负责检测可读队列，没有可读 socket 则阻塞进程
```

​	一个epoll对象都有一个独立的eventpoll结构体，用于存放通过epoll_ctl方法向epoll对象中添加进来的事件。这些事件都会挂载在红黑树中，如此，重复添加的事件就可以通过红黑树而高效的识别出来(红黑树的插入时间效率是lgn，其中n为红黑树元素个数)。

​	而所有添加到epoll中的事件都会与设备(网卡)驱动程序建立回调关系，也就是说，当相应的事件发生时会调用这个回调方法。这个回调方法在内核中叫ep_poll_callback,它会将发生的事件添加到rdlist双链表中。

​	当调用epoll_wait检查是否有事件发生时，只需要检查eventpoll对象中的rdlist双链表中是否有epitem元素即可。如果rdlist不为空，则把发生的事件复制到用户态，同时将事件数量返回给用户。 

![1627454668987](C:/Users/zxw/AppData/Roaming/Typora/typora-user-images/1627454668987.png)

#### epoll优点

- 没有最大并发连接的限制
- 使用mmap映射了共享空间，减少了内核与用户空间之间的数据复制开销
- 没有使用轮询的机制，不会随之连接数量的增加而影响效率，因此在实际网络环境中，epoll只会返回有效的fd，效率比select效率要更高

### Redis使用I/O多路复用技术的原因

由于Redis是单线程运行处理数据的读写，所有的操作都是按照时间顺序进行的，而读写操作等待用户的输入或输出都是阻塞的，所以I/O操作在一般情况下不会直接返回结果，这会导致因为某个文件的I/O阻塞而导致整个进程无法对其他客户提供服务，而I/O多路复用技术正好解决了这个问题。Redis的I/O模型主要是基于epoll实现的。



